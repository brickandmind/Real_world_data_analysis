{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "X_test_input = np.load('X_test_input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shape of X :'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2431135, 78)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'shape of y :'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2431135,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('shape of X :', np.shape(X_test),'shape of y :', np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = np.reshape(y_test, (-1, 1))\n",
    "y_train = np.reshape(y_train, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.56000000e+02,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.51712000e+05,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  1.56800000e+03,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.62478000e+05,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  7.17000000e+02,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.32896000e+05,   0.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  1.72000000e+02,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.09737000e+05,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  5.56000000e+02,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.60876000e+05,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  5.89000000e+02,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.87555000e+05,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan_to_num(X_train, copy=False)\n",
    "np.nan_to_num(X_test, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler  = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = X_train[~np.isnan(X_train).any(axis=1)]\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "X_test = X_test[~np.isnan(X_test).any(axis=1)]\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 78], name = 'x-input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name = 'y-input')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    \n",
    "    W1 = tf.get_variable(\"weight1\", shape=[78, 50],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([50]), name='bias1')\n",
    "    layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    w1_hist = tf.summary.histogram(\"weights1\", W1)\n",
    "    b1_hist = tf.summary.histogram(\"biases1\", b1)\n",
    "    layer1_hist = tf.summary.histogram(\"layer1\", layer1)\n",
    "    \n",
    "    \n",
    "    tf.add_to_collection('vars', W1)\n",
    "\n",
    "    layer1 = tf.nn.dropout(layer1, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "\n",
    "    W2 = tf.get_variable(\"weight2\", shape = [50, 25],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([25]), name='bias2')\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "    \n",
    "    \n",
    "    tf.add_to_collection('vars', W2)\n",
    "\n",
    "    w2_hist = tf.summary.histogram(\"weights2\", W2)\n",
    "    b2_hist = tf.summary.histogram(\"biases2\", b2)\n",
    "    layer2_hist = tf.summary.histogram(\"layer2\", layer2)\n",
    "    \n",
    "    layer2 = tf.nn.dropout(layer2, keep_prob=keep_prob)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    W3 = tf.get_variable(\"weight3\", shape = [25, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "    layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "    \n",
    "\n",
    "    tf.add_to_collection('vars', W3)\n",
    "\n",
    "    w3_hist = tf.summary.histogram(\"weights3\", W3)\n",
    "    b3_hist = tf.summary.histogram(\"biases3\", b3)\n",
    "    layer3_hist = tf.summary.histogram(\"layer3\", layer3)\n",
    "    \n",
    "    layer3 = tf.nn.dropout(layer3, keep_prob=keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    W4 = tf.get_variable(\"weight4\", shape = [10, 1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "    hypothesis = tf.matmul(layer3, W4) + b4\n",
    "    \n",
    "    tf.add_to_collection('vars', W4)\n",
    "\n",
    "    w4_hist = tf.summary.histogram(\"weights4\", W4)\n",
    "    b4_hist = tf.summary.histogram(\"biases4\", b4)\n",
    "    hypothesis_hist = tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# cost/loss function\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/recommendation\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        summary, _ = sess.run([merged_summary, train], feed_dict={X: X_train, Y: y_train,  keep_prob: 0.7})\n",
    "        writer.add_summary(summary, global_step=global_step)\n",
    "        global_step = 1\n",
    "        global_step +=10  \n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={\n",
    "                  X: X_train, Y: y_train}), sess.run([W1, W2, W3, W4]))\n",
    "\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: X_test, Y: y_test,  keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Trained Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.save(sess, 'trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Trained Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph('trained_model.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "all_vars = tf.get_collection('vars')\n",
    "for v in all_vars:\n",
    "    v_ = sess.run(v)\n",
    "    print(v_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Large Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer([...])\n",
    "reader = tf.TextLineReader()\n",
    "_, line = reader.read(filename_queue)\n",
    "\n",
    "line = tf.decode_csv(line, record_defaults=default)\n",
    "label_batch, feature_batch = tf.train.shuffle_batch([label, feature], batch_size=batch_size, capacity=512, min_after_dequeue=256,  num_threads=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
